{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs\n",
    "\n",
    "The objective of this competition is to decide if a pair of questions has the same meaning. The point is to help avoid question duplication on Quora. The evaluation method uses Log Loss, defined as\n",
    "\n",
    "$$ {\\rm Log\\_Loss} = -\\frac{1}{n} \\sum_{i=1}^n \\left(y_i \\log \\hat{y}_i + (1-y_i)\\log( 1-\\hat{y}_i) \\right) $$\n",
    "\n",
    "Here $y_i$ is the truth of whether pair number $i$ is a dupplicate (i.e., equal to either $0$ or $1$) and $\\hat{y}_i$ is the predicted probability that the pair is a duplicate. A perfect score is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import useful packages for data analysis and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(context = 'notebook', font_scale = 1.5, rc={'figure.figsize':(10, 6)})\n",
    "\n",
    "from IPython.display import display #for displaying multiple outputs from a single cell\n",
    "\n",
    "import gensim #for word2vec\n",
    "import nltk #natural language toolkit\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import sklearn\n",
    "import sklearn.neural_network\n",
    "import sklearn.model_selection\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "Import the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Features\n",
    "We will feed some features to our statistical model. In the interest of time, only a few features are used. More features would definitely be better. The features we use are mostly very simple, as well. We compute the number of words in each question, the number of characters, and the number of words the questions have in common. Slightly more sophisticated are the word2vec features. We use a pretrained word2vec model to associate a vector to each question, which is just the sum of the vectors associated to each word in a question (after removing stopwords). Then the two question vectors are compared using a few different distance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Load pretrained word2vec Model\n",
    "gnewsvectors = '~/Documents/Data Science/Data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(gnewsvectors, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute a Word2Vec vector associated to a question.\n",
    "def qVec(question):\n",
    "    #tokenize and remove stopwords\n",
    "    question = nltk.word_tokenize(str(question).lower().decode('utf-8'))\n",
    "    question = [word for word in question if word.isalpha()]\n",
    "    question = [word for word in question if word not in nltk.corpus.stopwords.words('english')]\n",
    "    #make a vector out of a question (as sum of vectors of words in question)\n",
    "    vec = []\n",
    "    for word in question:\n",
    "        try:\n",
    "            vec.append(word2vec[word])\n",
    "        except:\n",
    "            continue\n",
    "    vec = np.array(vec).sum(axis=0)\n",
    "    if type(vec) == np.float64:\n",
    "        vec = np.zeros(300) #return zero vector for those questions that are all stopwords\n",
    "    return vec\n",
    "\n",
    "#Construct Word2Vec vectors of questions in data and add them to the dataframe\n",
    "def makeWord2Vecs(data):\n",
    "    data['q1vec'] = data.question1.apply(lambda x: qVec(x))\n",
    "    data['q2vec'] = data.question2.apply(lambda x: qVec(x))\n",
    "    \n",
    "#Compute the number of words two questions have in common\n",
    "def numInCommon(question1, question2):\n",
    "    #Tokenize questions and remove stopwords\n",
    "    question1 = nltk.word_tokenize(str(question1).lower().decode('utf-8'))\n",
    "    question1 = [word for word in question1 if word.isalpha()]\n",
    "    question1 = [word for word in question1 if word not in nltk.corpus.stopwords.words('english')]  \n",
    "    question2 = nltk.word_tokenize(str(question2).lower().decode('utf-8'))\n",
    "    question2 = [word for word in question2 if word.isalpha()]\n",
    "    question2 = [word for word in question2 if word not in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "    if len(question1)==0 or len(question2) == 0:\n",
    "        return 0\n",
    "    \n",
    "    common_words = [word for word in question1 if word in question2]\n",
    "    \n",
    "    return len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#List of features for future reference\n",
    "features = ['q1Len','q2Len','q1NumWords','q2NumWords','wordsInCommon','euclidean', 'cosine', 'cityblock']\n",
    "#This function adds columns to the dataframe representing new features.\n",
    "def computeFeatures(data):\n",
    "    #####Simple Features\n",
    "    #Length of the question (in characters)\n",
    "    data['q1Len'] = data.question1.apply(lambda x: len(str(x)))\n",
    "    data['q2Len'] = data.question2.apply(lambda x: len(str(x)))\n",
    "    #Number of words in the question\n",
    "    data['q1NumWords'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "    data['q2NumWords'] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "    #Number of words in common\n",
    "    data['wordsInCommon'] = data.apply(lambda x: numInCommon(x['question1'],x['question2']) ,axis=1)\n",
    "    \n",
    "    \n",
    "    #####Word2Vec Distance Measures. Cut down on the number to save processing time.   \n",
    "    #Euclidean distance between questions\n",
    "    data['euclidean'] = data.apply(lambda x: scipy.spatial.distance.euclidean(x['q1vec'],x['q2vec']), axis=1)\n",
    "    #Cosine distance between questions\n",
    "    data['cosine'] = data.apply(lambda x: scipy.spatial.distance.cosine(x['q1vec'],x['q2vec']), axis=1)\n",
    "    #City block distance between questions\n",
    "    data['cityblock'] = data.apply(lambda x: scipy.spatial.distance.cityblock(x['q1vec'],x['q2vec']), axis=1)\n",
    "    #Canberra distance between questions\n",
    "    #data['canberra'] = data.apply(lambda x: scipy.spatial.distance.canberra(x['q1vec'],x['q2vec']), axis=1)\n",
    "    #Chebyshev distance between questions\n",
    "    #data['chebyshev'] = data.apply(lambda x: scipy.spatial.distance.chebyshev(x['q1vec'],x['q2vec']), axis=1)\n",
    "    #Minkowski p=3 distance between questions\n",
    "    #data['minkowski'] = data.apply(lambda x: scipy.spatial.distance.minkowski(x['q1vec'],x['q2vec'], 3), axis=1)\n",
    "    #Jaccard distance between questions\n",
    "    #data['jaccard'] = data.apply(lambda x: scipy.spatial.distance.jaccard(x['q1vec'],x['q2vec']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute word2vecs for each question\n",
    "makeWord2Vecs(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1vec</th>\n",
       "      <th>q2vec</th>\n",
       "      <th>q1Len</th>\n",
       "      <th>q2Len</th>\n",
       "      <th>q1NumWords</th>\n",
       "      <th>q2NumWords</th>\n",
       "      <th>wordsInCommon</th>\n",
       "      <th>euclidean</th>\n",
       "      <th>cosine</th>\n",
       "      <th>cityblock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.820496, 0.078125, -0.170593, -0.282959, -0...</td>\n",
       "      <td>[-0.586121, 0.149902, -0.181152, -0.609131, -0...</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3.708764</td>\n",
       "      <td>0.068972</td>\n",
       "      <td>50.923897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.296875, 0.263794, -0.0942383, -0.0251465, ...</td>\n",
       "      <td>[-0.575684, 0.543213, 0.283691, 0.909668, -0.7...</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>7.875035</td>\n",
       "      <td>0.512164</td>\n",
       "      <td>107.384239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.358398, -0.0273132, 0.311768, 0.163818, -0....</td>\n",
       "      <td>[0.00634766, 0.0675049, -0.274902, -0.0244141,...</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5.520098</td>\n",
       "      <td>0.222009</td>\n",
       "      <td>74.963737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0881348, 0.178955, -0.612556, 0.518799, -0...</td>\n",
       "      <td>[0.473511, -0.123413, 0.105713, 0.441162, -0.3...</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7.063795</td>\n",
       "      <td>0.650411</td>\n",
       "      <td>99.030556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.876587, 0.0933838, 0.897949, 0.224121, -0....</td>\n",
       "      <td>[-0.250488, 0.742676, 0.192139, 0.206299, -0.5...</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11.753539</td>\n",
       "      <td>0.369993</td>\n",
       "      <td>158.978638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                               q1vec  \\\n",
       "0  [-0.820496, 0.078125, -0.170593, -0.282959, -0...   \n",
       "1  [-0.296875, 0.263794, -0.0942383, -0.0251465, ...   \n",
       "2  [0.358398, -0.0273132, 0.311768, 0.163818, -0....   \n",
       "3  [-0.0881348, 0.178955, -0.612556, 0.518799, -0...   \n",
       "4  [-0.876587, 0.0933838, 0.897949, 0.224121, -0....   \n",
       "\n",
       "                                               q2vec  q1Len  q2Len  \\\n",
       "0  [-0.586121, 0.149902, -0.181152, -0.609131, -0...     66     57   \n",
       "1  [-0.575684, 0.543213, 0.283691, 0.909668, -0.7...     51     88   \n",
       "2  [0.00634766, 0.0675049, -0.274902, -0.0244141,...     73     59   \n",
       "3  [0.473511, -0.123413, 0.105713, 0.441162, -0.3...     50     65   \n",
       "4  [-0.250488, 0.742676, 0.192139, 0.206299, -0.5...     76     39   \n",
       "\n",
       "   q1NumWords  q2NumWords  wordsInCommon  euclidean    cosine   cityblock  \n",
       "0          14          12              6   3.708764  0.068972   50.923897  \n",
       "1           8          13              2   7.875035  0.512164  107.384239  \n",
       "2          14          10              2   5.520098  0.222009   74.963737  \n",
       "3          11           9              0   7.063795  0.650411   99.030556  \n",
       "4          13           7              2  11.753539  0.369993  158.978638  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeFeatures(train_data)\n",
    "train_data=train_data.dropna() #I didn't bother tracking down the source of the NaN. This would likely be useful.\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "Now we use the features we've created to predict whether we have duplicates. We will use a XGBoost to construct the model. One interesting twist is that the training data that was supplied has a much larger fraction of duplicates than the test data. The XGBoost classfier works better when the training data has the same fraction of positives as the positives. The fraction of positives in the test data is $p=0.17$, so we pad the negatives in the training data until the fraction of positives drops to $p$. This is done in a not-very-intelligent way, and there is a risk of overtraining, but doing this seems to improve the results. I'm not too worried about the details of this procedure, though, since it doesn't really have anything to do with the actual task of detecting duplicate questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.593421\tcv-logloss:0.593359\n",
      "[50]\ttrain-logloss:0.340231\tcv-logloss:0.340193\n",
      "[100]\ttrain-logloss:0.335793\tcv-logloss:0.336516\n",
      "[150]\ttrain-logloss:0.333456\tcv-logloss:0.334822\n",
      "[200]\ttrain-logloss:0.331439\tcv-logloss:0.333469\n",
      "[250]\ttrain-logloss:0.329867\tcv-logloss:0.332531\n",
      "[300]\ttrain-logloss:0.328442\tcv-logloss:0.331656\n",
      "[350]\ttrain-logloss:0.327298\tcv-logloss:0.331025\n",
      "[399]\ttrain-logloss:0.325989\tcv-logloss:0.33021\n"
     ]
    }
   ],
   "source": [
    "X = train_data[features]\n",
    "Y = train_data['is_duplicate']\n",
    "\n",
    "#Oversample negative results to get a total fraction p of positives.\n",
    "#The idea for this code snippet came from a comment on the message boards.\n",
    "p=0.17\n",
    "num_pos = len(X[Y==1])\n",
    "num_neg = len(X[Y==0])\n",
    "neg_results = X[Y==0]\n",
    "pos_results = X[Y==1]\n",
    "factor = num_pos/float(num_neg*p)-1\n",
    "while factor > 1:\n",
    "    neg_results = pd.concat([neg_results,X[Y==0]])\n",
    "    factor -=1\n",
    "neg_results = pd.concat([neg_results,X[Y==0][:int(factor*len(X[Y==0]))]])\n",
    "X = pd.concat([pos_results,neg_results]).reset_index(drop=True)\n",
    "Y = pd.concat([pd.Series(np.ones(num_pos)),pd.Series(np.zeros(len(neg_results)))]).reset_index(drop=True)   \n",
    "\n",
    "\n",
    "X_train, X_cv, Y_train, Y_cv = sklearn.model_selection.train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train,label = Y_train)\n",
    "dcv = xgb.DMatrix(X_cv,label = Y_cv)\n",
    "\n",
    "params = {'objective':'binary:logistic','eval_metric':'logloss','eta':0.2,'max_depth':4}\n",
    "watchlist = [(dtrain, 'train'), (dcv, 'cv')]\n",
    "\n",
    "bst = xgb.train(params, dtrain, 400, watchlist, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission\n",
    "\n",
    "Creating the submission is as simple as processing the test data like we did for the training data and running the XGBoost model on it. Because our data processing throws out some of the data (never got around to tracking down exactly what happened there), we will need to replace that data in the submission. Since we can't apply our model to it, we just guess that those entries are duplicates with probability $p=0.17$, since that is the fraction of duplicates in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.345796e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.172898e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.771731e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.864488e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.172898e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.759346e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.345795e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            test_id\n",
       "count  2.345796e+06\n",
       "mean   1.172898e+06\n",
       "std    6.771731e+05\n",
       "min    0.000000e+00\n",
       "25%    5.864488e+05\n",
       "50%    1.172898e+06\n",
       "75%    1.759346e+06\n",
       "max    2.345795e+06"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "all_ids = test_data['test_id'] #will need this list for later\n",
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>q1vec</th>\n",
       "      <th>q2vec</th>\n",
       "      <th>q1Len</th>\n",
       "      <th>q2Len</th>\n",
       "      <th>q1NumWords</th>\n",
       "      <th>q2NumWords</th>\n",
       "      <th>wordsInCommon</th>\n",
       "      <th>euclidean</th>\n",
       "      <th>cosine</th>\n",
       "      <th>cityblock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "      <td>[-0.114532, 0.275391, 0.631348, -0.134033, -0....</td>\n",
       "      <td>[0.0960693, 0.323242, 0.634277, 0.692871, -1.1...</td>\n",
       "      <td>57</td>\n",
       "      <td>68</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>8.468845</td>\n",
       "      <td>3.954517e-01</td>\n",
       "      <td>114.431046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "      <td>[0.0454102, 0.900635, 0.343994, 0.495117, -0.3...</td>\n",
       "      <td>[-0.325195, 0.468506, 0.366943, 0.397949, -0.5...</td>\n",
       "      <td>66</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3.817149</td>\n",
       "      <td>1.089034e-01</td>\n",
       "      <td>53.327644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "      <td>[-0.116577, 0.238281, 0.506485, 1.20508, -0.24...</td>\n",
       "      <td>[-0.172852, 0.0881348, 0.239807, 0.691406, -0....</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4.425264</td>\n",
       "      <td>1.636005e-01</td>\n",
       "      <td>62.257957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "      <td>[-0.218994, 0.065918, -0.291016, 0.28125, -0.5...</td>\n",
       "      <td>[-0.152344, 0.205078, 0.201172, 0.490234, -0.1...</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.878658</td>\n",
       "      <td>3.196543e-01</td>\n",
       "      <td>54.527912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "      <td>[0.0681152, 0.0792236, -0.119995, 0.458008, -0...</td>\n",
       "      <td>[0.0681152, 0.0792236, -0.119995, 0.458008, -0...</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.255779e-08</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \\\n",
       "0  Why did Microsoft choose core m3 and not core ...   \n",
       "1        How much cost does hair transplant require?   \n",
       "2                      What you send money to China?   \n",
       "3                                  What foods fibre?   \n",
       "4                     How their can I start reading?   \n",
       "\n",
       "                                               q1vec  \\\n",
       "0  [-0.114532, 0.275391, 0.631348, -0.134033, -0....   \n",
       "1  [0.0454102, 0.900635, 0.343994, 0.495117, -0.3...   \n",
       "2  [-0.116577, 0.238281, 0.506485, 1.20508, -0.24...   \n",
       "3  [-0.218994, 0.065918, -0.291016, 0.28125, -0.5...   \n",
       "4  [0.0681152, 0.0792236, -0.119995, 0.458008, -0...   \n",
       "\n",
       "                                               q2vec  q1Len  q2Len  \\\n",
       "0  [0.0960693, 0.323242, 0.634277, 0.692871, -1.1...     57     68   \n",
       "1  [-0.325195, 0.468506, 0.366943, 0.397949, -0.5...     66     43   \n",
       "2  [-0.172852, 0.0881348, 0.239807, 0.691406, -0....     60     29   \n",
       "3  [-0.152344, 0.205078, 0.201172, 0.490234, -0.1...     27     17   \n",
       "4  [0.0681152, 0.0792236, -0.119995, 0.458008, -0...     32     30   \n",
       "\n",
       "   q1NumWords  q2NumWords  wordsInCommon  euclidean        cosine   cityblock  \n",
       "0          11          14              3   8.468845  3.954517e-01  114.431046  \n",
       "1          14           7              4   3.817149  1.089034e-01   53.327644  \n",
       "2          14           6              3   4.425264  1.636005e-01   62.257957  \n",
       "3           4           3              0   3.878658  3.196543e-01   54.527912  \n",
       "4           4           6              2   0.000000  9.255779e-08    0.000000  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeWord2Vecs(test_data)\n",
    "computeFeatures(test_data)\n",
    "test_data=test_data.dropna()\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.084937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.331026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.303677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.002113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.221190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  is_duplicate\n",
       "0        0      0.084937\n",
       "1        1      0.331026\n",
       "2        2      0.303677\n",
       "3        3      0.002113\n",
       "4        4      0.221190"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_data[features]\n",
    "\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "probs = bst.predict(dtest)\n",
    "ids = test_data['test_id']\n",
    "preds = pd.DataFrame(columns = ['test_id','is_duplicate'])\n",
    "preds['test_id'] = ids\n",
    "preds['is_duplicate'] = probs\n",
    "#Now add missing test values back in.\n",
    "missing_ids = set(all_ids) - set(ids)\n",
    "missing_preds = pd.DataFrame(columns = ['test_id','is_duplicate'])\n",
    "missing_preds['test_id'] = list(missing_ids)\n",
    "missing_preds['is_duplicate'] = p\n",
    "preds = pd.concat([preds,missing_preds]).sort_values(by = 'test_id')\n",
    "preds.reset_index(drop=True, inplace=True)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save predictions to csv file for submission.\n",
    "#preds.to_csv('xgb-resample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
